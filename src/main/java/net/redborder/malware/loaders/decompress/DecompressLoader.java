package net.redborder.malware.loaders.decompress;

import net.redborder.decompress.UniversalDecompressor;
import net.redborder.decompress.models.ArchiveFile;
import net.redborder.malware.loaders.AbstractFileDroppingLoader;
import net.redborder.malware.util.kafka.KafkaManager;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.MRJobConfig;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.yarn.util.SystemClock;
import org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit;
import org.apache.pig.data.Tuple;
import org.codehaus.jackson.map.ObjectMapper;
import org.jets3t.service.Jets3tProperties;
import org.jets3t.service.S3Service;
import org.jets3t.service.S3ServiceException;
import org.jets3t.service.impl.rest.httpclient.RestS3Service;
import org.jets3t.service.model.S3Object;
import org.jets3t.service.security.AWSCredentials;

import java.io.ByteArrayInputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.IOException;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class DecompressLoader extends AbstractFileDroppingLoader{


    ObjectMapper mapper;
    S3Service s3Service;
    AWSCredentials awsCredentials;

    final String JETS3T_CONFIG_FILE = "jets3t.properties";

    public DecompressLoader(String zkHost, String timeoutMS, String useDevShm, String awsAcess, String awsSecret) {
        super(zkHost, timeoutMS, useDevShm);
        mapper = new ObjectMapper();
        awsCredentials = new AWSCredentials(awsAcess, awsSecret);
    }

    @Override
    public void prepareToRead(RecordReader reader, PigSplit split) throws IOException {
        super.prepareToRead(reader, split);
        Jets3tProperties properties = new Jets3tProperties();
        try {
            properties.loadAndReplaceProperties(new FileInputStream(new File(JETS3T_CONFIG_FILE)), JETS3T_CONFIG_FILE);
            s3Service = new RestS3Service(awsCredentials, null, null, properties);
        } catch (IOException e){
            e.printStackTrace();
        }
    }

    @Override
    public Tuple processFile(Text key, BytesWritable value, File binaryFile) throws IOException {
        Configuration configuration = new Configuration();
        configuration.setBoolean(MRJobConfig.MAPREDUCE_JOB_USER_CLASSPATH_FIRST, true);

        Map<String, Object> data = new HashMap<>();

        try {
            List<ArchiveFile> files = UniversalDecompressor.decompress(binaryFile);
            List<HashMap<String, String>> fileMaps = new ArrayList<>();
            if (files != null) {
                for (ArchiveFile file : files) {
                    file.calculateSha256();
                    // S3 file upload
                    S3Object fileObject = new S3Object();
                    fileObject.setDataInputStream(new ByteArrayInputStream(file.getContent()));
                    fileObject.setKey("/mdata/input/" + file.getSha256());
                    s3Service.putObject("malware", fileObject);

                    // JSON content generation
                    HashMap<String, String> fileMap = new HashMap<>();
                    fileMap.put("relative_path", file.getRelativePath());
                    fileMap.put("hash", file.getSha256());
                    fileMaps.add(fileMap);
                }
                data.put("files", fileMaps);
            }

            getProtoTuple().clear();
            String json = mapper.writeValueAsString(data);
            getProtoTuple().add(key.toString());
            getProtoTuple().add(json);
        } catch (IOException e){
            e.printStackTrace();
        } catch (S3ServiceException e){
            e.printStackTrace();
        }

        return getTupleFactory().newTuple(getProtoTuple());
    }

    /* Private methods */

}
